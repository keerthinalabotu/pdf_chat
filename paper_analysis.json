{
  "title": "Abstract",
  "authors": [],
  "abstract": "State-of-the-art object detection systems rely on an ac-\ncurate set of region proposals. Several recent methods use\na neural network architecture to hypothesize promising ob-\nject locations. While these approaches are computation-\nally ef\ufb01cient, they rely on \ufb01xed image regions as anchors\nfor predictions. In this paper we propose to use a search\nstrategy that adaptively directs computational resources to\nsub-regions likely to contain objects. Compared to meth-\nods based on \ufb01xed anchor locations, our approach natu-\nrally adapts to cases where object instances are sparse and\nsmall. Our approach is comparable in terms of accuracy\nto the state-of-the-art Faster R-CNN approach while using\ntwo orders of magnitude fewer anchors on average. Code is\npublicly available.\n1. Introduction\nObject detection is an important computer vision prob-\nlem for its intriguing challenges and large variety of ap-\nplications. Signi\ufb01cant recent progress in this area has\nbeen achieved by incorporating deep convolutional neu-\nral networks (DCNN) [15] into object detection systems\n[5, 8, 10, 12, 17, 23, 25].\nAn object detection algorithm with state-of-the-art ac-\ncuracy typically has the following two-step cascade: a set\nof class-independent region proposals are hypothesized an d\nare then used as input to a detector that gives each region\na class label. The role of region proposals is to reduce the\ncomplexity through limiting the number of regions that need\nbe evaluated by the detector. However, with recently intro-\nduced techniques that enable sharing of convolutional fea-\ntures [9, 12], traditional region proposal algorithms such as\nselective search [27] and EdgeBoxes [29] become the bot-\ntleneck of the detection pipeline.\nAn emerging class of ef\ufb01cient region proposal meth-ods are based on end-to-end trained deep neural networks\n[5, 22]. The common idea in these approaches is to train\na class-independent regressor on a small set of pre-de\ufb01ned\nanchor regions. More speci\ufb01cally, each anchor region is as-\nsigned the task of deciding whether an object is in its neigh-\nborhood (in terms of center location, scale and aspect ratio ),\nand predicting a bounding box for that object through re-\ngression if that is the case. The design of anchors differs fo r\neach method. For example, MultiBox [5] uses 800 anchors\nfrom clustering, YOLO [21] uses a non-overlapping 7 by 7\ngrid, RPN [22] uses overlapping sliding windows. In these\nprior works the test-time anchors are not adaptive to the ac-\ntual content of the images, thus to further improve accuracy\nfor detecting small object instances a denser grid of anchor s\nis required for all images, resulting in longer test time and a\nmore complex network model.\nWe alternatively consider the following adaptive search\nstrategy. Instead of \ufb01xing a priori a set of anchor regions,\nour algorithm starts with the entire image. It then recur-\nsively divides the image into sub-regions (see Figure 2) un-\ntil it decides that a given region is unlikely to enclose any\nsmall objects. The regions that are visited in the process\neffectively serve as anchors that are assigned the task of\npredicting bounding boxes for objects nearby. A salient\nfeature of our algorithm is that the decision of whether to\ndivide a region further is based on features extracted from\nthat particular region. As a result, the generation of the se t\nof anchor regions is conditioned on the image content. For\nan image with only a few small objects most regions are\npruned early in the search, leaving a few small anchor re-\ngions near the objects. For images that contain exclusively\nlarge instances, our approach gracefully falls back to exis t-\ning methods that rely on a small number of large anchor\nregions. In this manner, our algorithm adaptively directs i ts\ncomputational resources to regions that are likely to conta in\nobjects. Figure 1 compares our algorithm with RPN.\nTo support our adaptive search algorithm, we train a deep\n2351\n\nFigure 1. Comparison of our proposed adaptive search algori thm with the non-adaptive RPN method. The red boxes show regi on proposals\nfrom adjacency predictions. Note that for small objects, RP N is forced to perform regression from much larger anchors, w hile our AZ-Net\napproach can adaptively use features from small regions.\nneural network we call Adjacency and Zoom Network (AZ-\nNet). Given an input anchor region, the AZ-Net outputs a\nscalar zoom indicator which is used to decide whether to\nfurther zoom into (divide) the region and a set of bound-\ning boxes with con\ufb01dence scores, or adjacency predictions .\nThe adjacency predictions with high con\ufb01dence scores are\nthen used as region proposals for a subsequent object de-\ntector. The network is applied recursively starting from th e\nwhole image to generate an adaptive set of proposals.\nTo intuitively motivate the design of our network, con-\nsider a situation in which one needs to perform a quick\nsearch for a car. A good strategy is to \ufb01rst look for\nlarger structures that could provide evidence for existenc e\nof smaller structures in related categories. A search agent\ncould, for example, look for roads and use that to reason\nabout where cars should be. Once the search nears the car,\none could use the fact that seeing certain parts is highly pre -\ndictive of the spatial support of the whole. For instance,\nthe wheels provide strong evidence for a tight box of the\ncar. In our design, the zoom indicator mimics the process\nof searching for larger structures, while the adjacency pre -\ndictions mimic the process of neighborhood inference.\nTo validate this design we extensively evaluate our al-\ngorithm on Pascal VOC 2007 [6] with \ufb01ne-grained analy-\nsis. We also report baseline results on the recently intro-\nduced MSCOCO [18] dataset. Our algorithm achieves de-\ntection mAP that is close to state-of-the-art methods at a\nfast frame rate. Code has been made publicly available at\nhttps://github.com/luyongxi/az-net .\nIn summary, we make the following contributions:\n\u2022We design a search strategy for object detection that\nadaptively focuses computational resources on image\nregions that contain objects.\n\u2022We evaluate our approach on Pascal VOC 2007 and\nMSCOCO datasets and demonstrate it is comparableto Fast R-CNN and Faster R-CNN with fewer anchor\nand proposal regions.\n\u2022We provide a \ufb01ne-grained analysis that shows intrigu-\ning features of our approach. Namely, our proposal\nstrategy has better recall for higher intersection-over-\nunion thresholds, higher recall for smaller numbers of\ntop proposals, and for smaller object instances.\nThis paper is organized as follows. In section 2 we sur-\nvey existing literature highlighting the novelty of our ap-\nproach. In Section 3 we introduce the design of our algo-\nrithm. Section 4 presents an empirical comparison to exist-\ning object detection methods on standard evaluation bench-\nmarks, and Section 5 discusses possible future directions.\n2. Previous Work\nLampert et al . [16] \ufb01rst proposed an adaptive branch-\nand-bound approach. More recently, Gonzeles-Garcia et al.\n[11], Caicedo and Lazebnik [3], and Yoo et al. [28] explored\nactive object detection with DCNN features. While these\napproaches show the promise of using an adaptive algo-\nrithm for object detection, their detectors are class-wise and\ntheir methods cannot achieve competitive accuracy. Our ap-\nproach, on the other hand, is multi-class and is compara-\nble to state-of-the-art approaches in both accuracy and tes t\nspeed.\nThe idea of using spatial context has been previously ex-\nplored in the literature. Previous work by Torralba et al .\n[26] used a biologically inspired visual attention model [2 ],\nbut our focus is on ef\ufb01cient engineering design. Divvala et\nal. [4] evaluated the use of context for localization, but thei r\nempirical study was performed on hand-crafted features and\nneeds to be reexamined in combination with more accurate\nrecent approaches.\nOur method is closely related to recent approaches that\n2352\n\n1 2\n345\nFigure 2. As illustrated, a given region is divided into 5 sub -regions\n(numbered). Each of these sub-regions is recursively divid ed if its\nzoom indicator is above a threshold.\nNo zoom No zoom Zoom\nFigure 3. Illustration of desired zoom indicator for common situa-\ntions. The green boxes are objects, and the red boxes are regi ons.\nLeft: the object is small but it is mostly outside the region \u2013 there\nis no gain in zooming in. Middle: the object is mostly inside b ut\nits size is large relative to the region \u2013 there is no gain in zo oming\nin. Right: there is a small object that is completely inside t he re-\ngion. In this case further division of the region greatly inc reases\nthe chance of detection for that object.\nuse anchor regions for proposal generation or detection. Fo r\nexample, Erhan et al. [5] use 800 data-driven anchors for re-\ngion proposals and Redmon et al. [21] use a \ufb01xed grid of 49\nnon-overlapping regions to provide class-wise detections .\nThe former has the concern that these anchors could over-\n\ufb01t the data, while the latter cannot achieve state-of-the-a rt\nperformance without model ensembles. Our work is most\nrelated to the recent work by Ren et al. [22], which uses a set\nof heuristically designed 2400 overlapping anchor regions .\nOur approach uses a similar regression technique to predict\nmultiple bounding boxes from an anchor region. However,\nour anchor regions are generated adaptively, making them\nintrinsically more ef\ufb01cient. In particular, we show that it is\npossible to detect small object instances in the scene with-\nout an excessive number of anchor regions. We propose to\ngrow a tree of \ufb01ner-grained anchor regions based on local\nimage evidence, and design the regression model strategi-\ncally on top of it. We extensively compare the output of our\nmethod against [22] in our experimental section and show\nthe unique advantages of our approach.\nThis paper is a follow-up to the work published in the\n53rd Annual Allerton Conference [20]. Here, we introduce\na substantially improved algorithm and add extensive eval-\nuations on standard benchmarks.1 231\n23\n12\n3\n4\nFigure 4. Illustration of sub-region priors. From left to ri ght: ver-\ntical stripes, horizontal stripes, neighboring squares. T he red rect-\nangular box is the image. In the \ufb01gure the numbered regions ar e\ntemplate sub-regions. The gaps between sub-regions are exa gger-\nated for better visualization. The vertical stripes are use d to detect\ntall objects, the horizontal stripes are used to detect fat o bjects,\nwhile the neighboring squares are used to detect objects tha t fall\nin the gaps between anchor regions generated in the search pr o-\ncess.\n3. Design of the Algorithm\n3.1. Overview of the Adaptive Search\nOur object detection algorithm consists of two steps. In\nstep 1, a set of class-independent region proposals are gen-\nerated using Adaptive Search with AZ-Net (see Algorithm\n1). In step 2, an object detector evaluates each region pro-\nposed in step 1 to provide class-wise detections. In our ex-\nperiments the detector is Fast R-CNN.\nOur focus is on improving step 1. We consider a recur-\nsive search strategy, starting from the entire image as the\nroot region. For any region encountered in the search pro-\ncedure, the algorithm extracts features from this region to\ncompute the zoom indicator and the adjacency predictions.\nThe adjacency predictions with con\ufb01dence scores above a\nthreshold are included in the set of output region proposals .\nIf the zoom indicator is above a threshold, this indicates\nthat the current region is likely to contain small objects. T o\ndetect these embedded small objects, the current region is\ndivided into sub-regions in the manner shown in Figure 2.\nEach of these sub-regions is then recursively processed in\nthe same manner as its parent region, until either its area\nor its zoom indicator is too small. Figure 1 illustrates this\nprocedure.\nIn the following section, we discuss the design of the\nzoom indicator and adjacency prediction.\n3.2. Design of Building Blocks\nThe zoom indicator should be large for a region only\nwhen there exists at least one object whose spatial support\nmostly lies within the region, and whose size is suf\ufb01ciently\nsmall compared to the region. The reasoning is that we\nshould zoom in to a region only when it substantially in-\ncreases the chance of detection. For example, if an object is\nmostly outside the region, dividing the region further is un -\nlikely to increase the chance of detecting that object. Sim-\nilarly, if an object is large compared to the current region,\nthe task of detecting this object should be handled by this\n2353\n\nAlgorithm 1: Adaptive search with AZ-Net.\nData : Input image x(the whole image region bx).Ykis the region proposed at step k.Ykare the accumulated region\nproposals up to step k.Zkare the regions to further zoom in to at step k.Bkare anchor regions at step k.\nResult : Region proposals at termination YK.\nInitialization: B0\u2190{bx}.Y0\u2190\u2205,k\u21900\nwhile (Bkis not an empty set) do\nInitialize YkandZkas empty sets.\nforeachb\u2208Bkdo\nCompute adjacency predictions Aband the zoom indicator zbusing AZ-Net.\nInclude all a\u2208Abwith high con\ufb01dence scores into Yk.\nIncludebintoZkifzbis above threshold.\nend\nYk\u2190Yk\u22121\u222aYk\nBk+1\u2190Divide-Regions (Zk)\nk\u2190k+1\nend\nK\u2190k\u22121\nFigure 5. Illustration of the AZ-Net architecture.\nregion or its parents. In the latter case, further division o f\nthe region not only wastes computational resources, but als o\nintroduces false positives in the region proposals. Figure 3\nshows common situations and the desirable behavior of the\nzoom indicator.\nThe role of adjacency prediction is to detect one or mul-\ntiple objects that overlap with the anchor region suf\ufb01cient ly\nby providing tight bounding boxes. The adjacency predic-\ntion should be aware of the search geometry induced by the\nzoom indicator. More speci\ufb01cally, the adjacency predic-\ntion should perform well on the effective anchor regions in-\nduced by the search algorithm. For this purpose we propose\na training procedure that is aware of the adaptive search\nscheme (discussed in Section 3.3). On the other hand, its\ndesign should explicitly account for typical geometric con -\n\ufb01gurations of objects that fall inside the region, so that th e\ntraining can be performed in a consistent fashion. For this\nreason, we propose to make predictions based on a set of\nsub-region priors as shown in Figure 4. Note that we also\ninclude the anchor region itself as an additional prior. Wemake sub-region priors large compared to the anchor under\nthe intuition that if an object is small, it is best to wait unt il\nthe features extracted are at the right scale to make bound-\ning box predictions.\n3.3. Implementation\nWe implement our algorithm using the Caffe [14] frame-\nwork, utilizing the open source infrastructure provided by\nthe Fast R-CNN repository [9]. In this section we intro-\nduce the implementation details of our approach. We use\nthe Fast R-CNN detector since it is a fast and accurate re-\ncent approach. Our method should in principle work for a\nbroad class of object detectors that use region proposals.\nWe train a deep neural network as illustrated in Figure\n5. Note that in addition to the sub-region priors as shown\nin Figure 4, we also add the region itself as a special prior\nregion, making in total 11adjacency predictions per anchor.\nFor the convolutional layers, we use the VGG16 model [23]\npre-trained on ImageNet data. The fully-connected layers\nare on top of a region pooling layer introduced in [9] which\nallows ef\ufb01cient sharing of convolutional layer features.\nThe training is performed as a three-step procedure.\nFirst, a set of regions is sampled from the image. These\nsamples should contain hard positive and negative exam-\nples for both the zoom indicator and the adjacency predic-\ntion. Finally, the tuples of samples and labels are used in\nstandard stochastic gradient descent training. We now dis-\ncuss how the regions are sampled and labeled, and the loss\nfunction we choose.\n3.3.1 Region Sampling and Labeling\nSince a typical image only has a few object instances, to\nprovide suf\ufb01cient positive examples for adjacency predic-\n2354\n\nFigure 6. Illustration of the inverse matching procedure. T he red\nbox is the inverse match for the object (green box). The left \ufb01 gure\nshows inverse matching of a neighboring square, the right \ufb01g ure\nshows inverse matching of a vertical stripe.\ntions our method inversely \ufb01nds regions that will see a\nground truth object as a perfect \ufb01t to its prior sub-regions\n(see Figure 6 for illustration). This provides k\u00d711training\nexamples for each image, where kis the number of objects.\nTo mine for negative examples and hard positive exam-\nples, we search the input image as in Algorithm 1. Note\nthat the algorithm uses zoom indicators from the AZ-Net.\nInstead of optimizing AZ-Net with an on-policy approach\n(that uses the intermediate AZ-Net model to sample re-\ngions), which might cause training to diverge, we replace\nthe zoom prediction with the zoom indicator label. How-\never, we note that using the zoom label directly could cause\nover\ufb01tting, since at test time the algorithm might encounte r\nsituations where a previous zoom prediction is wrong. To\nimprove the robustness of the model, we add noise to the\nzoom label by \ufb02ipping the ground truth with a probability\nof0.3. We found that models trained without random \ufb02ip-\nping are signi\ufb01cantly less accurate. For each input image\nwe initiate this procedure with \ufb01ve sub-images and repeat it\nmultiple times. We also append horizontally \ufb02ipped images\nto the dataset for data augmentation.\nAssignment of labels for the zoom indicator follows the",
  "sections": [
    {
      "title": "Abstract",
      "content": "State-of-the-art object detection systems rely on an ac-\ncurate set of region proposals. Several recent methods use\na neural network architecture to hypothesize promising ob-\nject locations. While these approaches are computation-\nally ef\ufb01cient, they rely on \ufb01xed image regions as anchors\nfor predictions. In this paper we propose to use a search\nstrategy that adaptively directs computational resources to\nsub-regions likely to contain objects. Compared to meth-\nods based on \ufb01xed anchor locations, our approach natu-\nrally adapts to cases where object instances are sparse and\nsmall. Our approach is comparable in terms of accuracy\nto the state-of-the-art Faster R-CNN approach while using\ntwo orders of magnitude fewer anchors on average. Code is\npublicly available.\n1. Introduction\nObject detection is an important computer vision prob-\nlem for its intriguing challenges and large variety of ap-\nplications. Signi\ufb01cant recent progress in this area has\nbeen achieved by incorporating deep convolutional neu-\nral networks (DCNN) [15] into object detection systems\n[5, 8, 10, 12, 17, 23, 25].\nAn object detection algorithm with state-of-the-art ac-\ncuracy typically has the following two-step cascade: a set\nof class-independent region proposals are hypothesized an d\nare then used as input to a detector that gives each region\na class label. The role of region proposals is to reduce the\ncomplexity through limiting the number of regions that need\nbe evaluated by the detector. However, with recently intro-\nduced techniques that enable sharing of convolutional fea-\ntures [9, 12], traditional region proposal algorithms such as\nselective search [27] and EdgeBoxes [29] become the bot-\ntleneck of the detection pipeline.\nAn emerging class of ef\ufb01cient region proposal meth-ods are based on end-to-end trained deep neural networks\n[5, 22]. The common idea in these approaches is to train\na class-independent regressor on a small set of pre-de\ufb01ned\nanchor regions. More speci\ufb01cally, each anchor region is as-\nsigned the task of deciding whether an object is in its neigh-\nborhood (in terms of center location, scale and aspect ratio ),\nand predicting a bounding box for that object through re-\ngression if that is the case. The design of anchors differs fo r\neach method. For example, MultiBox [5] uses 800 anchors\nfrom clustering, YOLO [21] uses a non-overlapping 7 by 7\ngrid, RPN [22] uses overlapping sliding windows. In these\nprior works the test-time anchors are not adaptive to the ac-\ntual content of the images, thus to further improve accuracy\nfor detecting small object instances a denser grid of anchor s\nis required for all images, resulting in longer test time and a\nmore complex network model.\nWe alternatively consider the following adaptive search\nstrategy. Instead of \ufb01xing a priori a set of anchor regions,\nour algorithm starts with the entire image. It then recur-\nsively divides the image into sub-regions (see Figure 2) un-\ntil it decides that a given region is unlikely to enclose any\nsmall objects. The regions that are visited in the process\neffectively serve as anchors that are assigned the task of\npredicting bounding boxes for objects nearby. A salient\nfeature of our algorithm is that the decision of whether to\ndivide a region further is based on features extracted from\nthat particular region. As a result, the generation of the se t\nof anchor regions is conditioned on the image content. For\nan image with only a few small objects most regions are\npruned early in the search, leaving a few small anchor re-\ngions near the objects. For images that contain exclusively\nlarge instances, our approach gracefully falls back to exis t-\ning methods that rely on a small number of large anchor\nregions. In this manner, our algorithm adaptively directs i ts\ncomputational resources to regions that are likely to conta in\nobjects. Figure 1 compares our algorithm with RPN.\nTo support our adaptive search algorithm, we train a deep\n2351\n\nFigure 1. Comparison of our proposed adaptive search algori thm with the non-adaptive RPN method. The red boxes show regi on proposals\nfrom adjacency predictions. Note that for small objects, RP N is forced to perform regression from much larger anchors, w hile our AZ-Net\napproach can adaptively use features from small regions.\nneural network we call Adjacency and Zoom Network (AZ-\nNet). Given an input anchor region, the AZ-Net outputs a\nscalar zoom indicator which is used to decide whether to\nfurther zoom into (divide) the region and a set of bound-\ning boxes with con\ufb01dence scores, or adjacency predictions .\nThe adjacency predictions with high con\ufb01dence scores are\nthen used as region proposals for a subsequent object de-\ntector. The network is applied recursively starting from th e\nwhole image to generate an adaptive set of proposals.\nTo intuitively motivate the design of our network, con-\nsider a situation in which one needs to perform a quick\nsearch for a car. A good strategy is to \ufb01rst look for\nlarger structures that could provide evidence for existenc e\nof smaller structures in related categories. A search agent\ncould, for example, look for roads and use that to reason\nabout where cars should be. Once the search nears the car,\none could use the fact that seeing certain parts is highly pre -\ndictive of the spatial support of the whole. For instance,\nthe wheels provide strong evidence for a tight box of the\ncar. In our design, the zoom indicator mimics the process\nof searching for larger structures, while the adjacency pre -\ndictions mimic the process of neighborhood inference.\nTo validate this design we extensively evaluate our al-\ngorithm on Pascal VOC 2007 [6] with \ufb01ne-grained analy-\nsis. We also report baseline results on the recently intro-\nduced MSCOCO [18] dataset. Our algorithm achieves de-\ntection mAP that is close to state-of-the-art methods at a\nfast frame rate. Code has been made publicly available at\nhttps://github.com/luyongxi/az-net .\nIn summary, we make the following contributions:\n\u2022We design a search strategy for object detection that\nadaptively focuses computational resources on image\nregions that contain objects.\n\u2022We evaluate our approach on Pascal VOC 2007 and\nMSCOCO datasets and demonstrate it is comparableto Fast R-CNN and Faster R-CNN with fewer anchor\nand proposal regions.\n\u2022We provide a \ufb01ne-grained analysis that shows intrigu-\ning features of our approach. Namely, our proposal\nstrategy has better recall for higher intersection-over-\nunion thresholds, higher recall for smaller numbers of\ntop proposals, and for smaller object instances.\nThis paper is organized as follows. In section 2 we sur-\nvey existing literature highlighting the novelty of our ap-\nproach. In Section 3 we introduce the design of our algo-\nrithm. Section 4 presents an empirical comparison to exist-\ning object detection methods on standard evaluation bench-\nmarks, and Section 5 discusses possible future directions.\n2. Previous Work\nLampert et al . [16] \ufb01rst proposed an adaptive branch-\nand-bound approach. More recently, Gonzeles-Garcia et al.\n[11], Caicedo and Lazebnik [3], and Yoo et al. [28] explored\nactive object detection with DCNN features. While these\napproaches show the promise of using an adaptive algo-\nrithm for object detection, their detectors are class-wise and\ntheir methods cannot achieve competitive accuracy. Our ap-\nproach, on the other hand, is multi-class and is compara-\nble to state-of-the-art approaches in both accuracy and tes t\nspeed.\nThe idea of using spatial context has been previously ex-\nplored in the literature. Previous work by Torralba et al .\n[26] used a biologically inspired visual attention model [2 ],\nbut our focus is on ef\ufb01cient engineering design. Divvala et\nal. [4] evaluated the use of context for localization, but thei r\nempirical study was performed on hand-crafted features and\nneeds to be reexamined in combination with more accurate\nrecent approaches.\nOur method is closely related to recent approaches that\n2352\n\n1 2\n345\nFigure 2. As illustrated, a given region is divided into 5 sub -regions\n(numbered). Each of these sub-regions is recursively divid ed if its\nzoom indicator is above a threshold.\nNo zoom No zoom Zoom\nFigure 3. Illustration of desired zoom indicator for common situa-\ntions. The green boxes are objects, and the red boxes are regi ons.\nLeft: the object is small but it is mostly outside the region \u2013 there\nis no gain in zooming in. Middle: the object is mostly inside b ut\nits size is large relative to the region \u2013 there is no gain in zo oming\nin. Right: there is a small object that is completely inside t he re-\ngion. In this case further division of the region greatly inc reases\nthe chance of detection for that object.\nuse anchor regions for proposal generation or detection. Fo r\nexample, Erhan et al. [5] use 800 data-driven anchors for re-\ngion proposals and Redmon et al. [21] use a \ufb01xed grid of 49\nnon-overlapping regions to provide class-wise detections .\nThe former has the concern that these anchors could over-\n\ufb01t the data, while the latter cannot achieve state-of-the-a rt\nperformance without model ensembles. Our work is most\nrelated to the recent work by Ren et al. [22], which uses a set\nof heuristically designed 2400 overlapping anchor regions .\nOur approach uses a similar regression technique to predict\nmultiple bounding boxes from an anchor region. However,\nour anchor regions are generated adaptively, making them\nintrinsically more ef\ufb01cient. In particular, we show that it is\npossible to detect small object instances in the scene with-\nout an excessive number of anchor regions. We propose to\ngrow a tree of \ufb01ner-grained anchor regions based on local\nimage evidence, and design the regression model strategi-\ncally on top of it. We extensively compare the output of our\nmethod against [22] in our experimental section and show\nthe unique advantages of our approach.\nThis paper is a follow-up to the work published in the\n53rd Annual Allerton Conference [20]. Here, we introduce\na substantially improved algorithm and add extensive eval-\nuations on standard benchmarks.1 231\n23\n12\n3\n4\nFigure 4. Illustration of sub-region priors. From left to ri ght: ver-\ntical stripes, horizontal stripes, neighboring squares. T he red rect-\nangular box is the image. In the \ufb01gure the numbered regions ar e\ntemplate sub-regions. The gaps between sub-regions are exa gger-\nated for better visualization. The vertical stripes are use d to detect\ntall objects, the horizontal stripes are used to detect fat o bjects,\nwhile the neighboring squares are used to detect objects tha t fall\nin the gaps between anchor regions generated in the search pr o-\ncess.\n3. Design of the Algorithm\n3.1. Overview of the Adaptive Search\nOur object detection algorithm consists of two steps. In\nstep 1, a set of class-independent region proposals are gen-\nerated using Adaptive Search with AZ-Net (see Algorithm\n1). In step 2, an object detector evaluates each region pro-\nposed in step 1 to provide class-wise detections. In our ex-\nperiments the detector is Fast R-CNN.\nOur focus is on improving step 1. We consider a recur-\nsive search strategy, starting from the entire image as the\nroot region. For any region encountered in the search pro-\ncedure, the algorithm extracts features from this region to\ncompute the zoom indicator and the adjacency predictions.\nThe adjacency predictions with con\ufb01dence scores above a\nthreshold are included in the set of output region proposals .\nIf the zoom indicator is above a threshold, this indicates\nthat the current region is likely to contain small objects. T o\ndetect these embedded small objects, the current region is\ndivided into sub-regions in the manner shown in Figure 2.\nEach of these sub-regions is then recursively processed in\nthe same manner as its parent region, until either its area\nor its zoom indicator is too small. Figure 1 illustrates this\nprocedure.\nIn the following section, we discuss the design of the\nzoom indicator and adjacency prediction.\n3.2. Design of Building Blocks\nThe zoom indicator should be large for a region only\nwhen there exists at least one object whose spatial support\nmostly lies within the region, and whose size is suf\ufb01ciently\nsmall compared to the region. The reasoning is that we\nshould zoom in to a region only when it substantially in-\ncreases the chance of detection. For example, if an object is\nmostly outside the region, dividing the region further is un -\nlikely to increase the chance of detecting that object. Sim-\nilarly, if an object is large compared to the current region,\nthe task of detecting this object should be handled by this\n2353\n\nAlgorithm 1: Adaptive search with AZ-Net.\nData : Input image x(the whole image region bx).Ykis the region proposed at step k.Ykare the accumulated region\nproposals up to step k.Zkare the regions to further zoom in to at step k.Bkare anchor regions at step k.\nResult : Region proposals at termination YK.\nInitialization: B0\u2190{bx}.Y0\u2190\u2205,k\u21900\nwhile (Bkis not an empty set) do\nInitialize YkandZkas empty sets.\nforeachb\u2208Bkdo\nCompute adjacency predictions Aband the zoom indicator zbusing AZ-Net.\nInclude all a\u2208Abwith high con\ufb01dence scores into Yk.\nIncludebintoZkifzbis above threshold.\nend\nYk\u2190Yk\u22121\u222aYk\nBk+1\u2190Divide-Regions (Zk)\nk\u2190k+1\nend\nK\u2190k\u22121\nFigure 5. Illustration of the AZ-Net architecture.\nregion or its parents. In the latter case, further division o f\nthe region not only wastes computational resources, but als o\nintroduces false positives in the region proposals. Figure 3\nshows common situations and the desirable behavior of the\nzoom indicator.\nThe role of adjacency prediction is to detect one or mul-\ntiple objects that overlap with the anchor region suf\ufb01cient ly\nby providing tight bounding boxes. The adjacency predic-\ntion should be aware of the search geometry induced by the\nzoom indicator. More speci\ufb01cally, the adjacency predic-\ntion should perform well on the effective anchor regions in-\nduced by the search algorithm. For this purpose we propose\na training procedure that is aware of the adaptive search\nscheme (discussed in Section 3.3). On the other hand, its\ndesign should explicitly account for typical geometric con -\n\ufb01gurations of objects that fall inside the region, so that th e\ntraining can be performed in a consistent fashion. For this\nreason, we propose to make predictions based on a set of\nsub-region priors as shown in Figure 4. Note that we also\ninclude the anchor region itself as an additional prior. Wemake sub-region priors large compared to the anchor under\nthe intuition that if an object is small, it is best to wait unt il\nthe features extracted are at the right scale to make bound-\ning box predictions.\n3.3. Implementation\nWe implement our algorithm using the Caffe [14] frame-\nwork, utilizing the open source infrastructure provided by\nthe Fast R-CNN repository [9]. In this section we intro-\nduce the implementation details of our approach. We use\nthe Fast R-CNN detector since it is a fast and accurate re-\ncent approach. Our method should in principle work for a\nbroad class of object detectors that use region proposals.\nWe train a deep neural network as illustrated in Figure\n5. Note that in addition to the sub-region priors as shown\nin Figure 4, we also add the region itself as a special prior\nregion, making in total 11adjacency predictions per anchor.\nFor the convolutional layers, we use the VGG16 model [23]\npre-trained on ImageNet data. The fully-connected layers\nare on top of a region pooling layer introduced in [9] which\nallows ef\ufb01cient sharing of convolutional layer features.\nThe training is performed as a three-step procedure.\nFirst, a set of regions is sampled from the image. These\nsamples should contain hard positive and negative exam-\nples for both the zoom indicator and the adjacency predic-\ntion. Finally, the tuples of samples and labels are used in\nstandard stochastic gradient descent training. We now dis-\ncuss how the regions are sampled and labeled, and the loss\nfunction we choose.\n3.3.1 Region Sampling and Labeling\nSince a typical image only has a few object instances, to\nprovide suf\ufb01cient positive examples for adjacency predic-\n2354\n\nFigure 6. Illustration of the inverse matching procedure. T he red\nbox is the inverse match for the object (green box). The left \ufb01 gure\nshows inverse matching of a neighboring square, the right \ufb01g ure\nshows inverse matching of a vertical stripe.\ntions our method inversely \ufb01nds regions that will see a\nground truth object as a perfect \ufb01t to its prior sub-regions\n(see Figure 6 for illustration). This provides k\u00d711training\nexamples for each image, where kis the number of objects.\nTo mine for negative examples and hard positive exam-\nples, we search the input image as in Algorithm 1. Note\nthat the algorithm uses zoom indicators from the AZ-Net.\nInstead of optimizing AZ-Net with an on-policy approach\n(that uses the intermediate AZ-Net model to sample re-\ngions), which might cause training to diverge, we replace\nthe zoom prediction with the zoom indicator label. How-\never, we note that using the zoom label directly could cause\nover\ufb01tting, since at test time the algorithm might encounte r\nsituations where a previous zoom prediction is wrong. To\nimprove the robustness of the model, we add noise to the\nzoom label by \ufb02ipping the ground truth with a probability\nof0.3. We found that models trained without random \ufb02ip-\nping are signi\ufb01cantly less accurate. For each input image\nwe initiate this procedure with \ufb01ve sub-images and repeat it\nmultiple times. We also append horizontally \ufb02ipped images\nto the dataset for data augmentation.\nAssignment of labels for the zoom indicator follows the",
      "start_page": 0,
      "end_page": 0
    },
    {
      "title": "discussion of Section 3. The label is 1 if there exists an ob-",
      "content": "ject with 50% of its area inside the region and the area is\nat most 25% of the size of the region. Note that here we\nuse a loose de\ufb01nition of inclusion to add robustness for ob-\njects falling between boundaries of anchors. For adjacency\nprediction, we set a threshold in the intersection-over-un ion\n(IoU) score between an object and a region. A region is\nassigned to detect objects with which it has suf\ufb01cient over-\nlap. The assigned objects are then greedily matched to one\nof the sub-regions de\ufb01ned by the priors shown in Figure 4.\nThe priority in the matching is determined by the IoU score\nbetween the objects and the sub-regions. We note that in\nthis manner multiple predictions from a region are possible .\n3.3.2 Loss Function\nAs shown in Figure 5, the AZ-Net has three output layers.\nThe zoom indicator outputs from a sigmoid activation func-\ntion. To train it we use the cross-entropy loss function pop-\nular for binary classi\ufb01cation. For the adjacency predictio ns,\nthe bounding boxes are parameterized as in Fast R-CNN[22]. Unlike in Fast R-CNN, to provide multiple predic-\ntions from any region, the con\ufb01dence scores are not nor-\nmalized to a probability vector. Correspondingly we use\nsmooth L1-loss for bounding box output and element-wise\ncross-entropy loss for con\ufb01dence score output. The three\nlosses are summed together to form a multi-task loss func-\ntion.\n3.3.3 Fast R-CNN Detectors\nThe detectors we use to evaluate proposal regions are Fast\nR-CNN detectors trained using AZ-Net proposals. As in\n[22], we implement two versions: one with unshared con-\nvolutional features and the other that shares convolutiona l\nfeatures with AZ-Net. The shared version is trained using\nalternating optimization.\n4. Experiments\nWe evaluate our approach on Pascal VOC 2007 [6] and\nMSCOCO [18] datasets. In addition to evaluating the accu-\nracy of the \ufb01nal detectors, we also perform detailed compar-\nisons between the RPN approach adopted in Faster R-CNN\nand our AZ-Net on VOC 2007. At the end of the section,\nwe give an analysis of the ef\ufb01ciency of our adaptive search\nstrategy.\n4.1. Results on VOC 2007\nTo set up a baseline comparison, we evaluate our ap-\nproach using the standard average precision (AP) metric\nfor object detection. For AP evaluation we use the de-\nvelopment kit provided by the VOC 2007 object detection\nchallenge. We compare our approach against the recently\nintroduced Fast R-CNN [9] and Faster R-CNN [22] sys-\ntems, which achieve state-of-the-art performance in stan-\ndard benchmarks, such as VOC 2007 [6] and VOC 2012\n[7]. A comparison is shown in Table 1. The results sug-\ngest that our approach is comparable to or better than these",
      "start_page": 0,
      "end_page": 0
    },
    {
      "title": "methods.",
      "content": "4.2. Quality of Region Proposals\nWe preform a detailed analysis of the quality of region\nproposals from our AZ-Net, highlighting a comparison to\nthe RPN network used in Faster R-CNN. For all our ex-\nperiments, we analyze the recall on Pascal VOC 2007 test\nset using the following de\ufb01nition: An object is counted as\nretrieved if there exists a region proposal with an above-\nthreshold IoU with it. The recall is then calculated as the\nproportion of the retrieved objects among all ground truth\nobject instances. To accurately reproduce the RPN ap-\nproach, we downloaded the region proposals provided on\nthe Faster R-CNN repository1. We used the results from\n1https://github.com/ShaoqingRen/faster_rcnn\n2355\n\nFigure 7. Example outputs of our algorithm. The left column s hows the original image. The middle column shows the anchor r egions\ninduced by our adaptive search. The right column shows the to p 100 adjacency predictions made around the anchor regions. The anchor\nregions and the adjacency predictions are superimposed int o a \ufb01gure at the same resolution of the original image. We note that the anchor\nregions and the region proposals in our approach are shared a cross object categories. For example, for the last image, th e algorithm\ngenerates anchor regions at proper scales near the dogs, the person, and the bottles.\nMethod boxes mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv\nAZ-Net 231 70.2 73.3 78.8 69.2 59.9 48.7 81.4 82.8 83.6 47.5 77.3 62.9 81.1 83.5 78.0 75.8 38.0 68.7 67.2 79.0 66.4\nAZ-Net* 228 70.4 73.9 79.9 68.8 58.9 49.1 80.8 83.3 83.7 47.2 75.8 63.8 80.6 84.4 78.9 75.8 39.2 70.2 67.4 78.4 68.3\nRPN 300 69.9 70.0 80.6 70.1 57.3 49.9 78.2 80.4 82.0 52.2 75.3 67.2 80.3 79.8 75.0 76.3 39.1 68.3 67.3 81.1 67.6\nRPN* 300 68.5 74.1 77.2 67.7 53.9 51.0 75.1 79.2 78.9 50.7 78.0 61.1 79.1 81.9 72.2 75.9 37.2 71.4 62.5 77.4 66.4\nFRCNN 2000 68.1 74.6 79.0 68.6 57.0 39.3 79.5 78.6 81.9 48.0 74.0 67.4 80.5 80.7 74.1 69.6 31.8 67.1 68.4 75.3 65.5\nTable 1. Comparison on VOC 2007 test set using VGG-16 for conv olutional layers. The results of RPN are reported in [22]. Th e results\nfor Fast R-CNN are reported in [9]. The AZ-Net and RPN results are reported for top-300 region proposals, but in AZ-Net man y images\nhave too few anchors to generate 300 proposals. * indicates r esults without shared convolutional features. All listed m ethods use DCNN\nmodels trained on VOC 2007 trainval.\na model reportedly trained on VOC 2007 trainval. Cor-\nrespondingly we compare it against our model trained on\nVOC 2007 trainval set. The comparisons concerning top-N regions are performed by ranking the region proposals in\norder of their con\ufb01dence scores.\nFigure 8 shows a comparison of recall at different IoU\n2356\n\n0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.9500.20.40.60.81\nThreshold at IoURecall at Top\u2212300 Region Proposals AZ\u2212Net (Ours)\nRPN (Faster R\u2212CNN)\nFigure 8. Comparison of recall of region proposals generate d by\nAZ-Net and RPN at different intersection over union thresho lds\non VOC 2007 test. The comparison is performed at top-300 re-\ngion proposals. Our approach has better recall at large IoU t hresh-\nolds, which suggests that AZ-Net proposals are more accurat e in\nlocalizing the objects.\nSmall Medium Large05101520\nSize of Ground Truth BoxesNumber of Matching Proposals AZ\u2212Net (Ours)\nRPN (Faster R\u2212CNN)\nFigure 9. Number of proposals matched to ground truth (with\nIoU= 0.5). This shows proposals from AZ-Net are more con-\ncentrated around true object locations.\n10010110200.20.40.60.81\nNumber of Region ProposalsRecall at IoU Threshold = 0.50 \nAZ\u2212Net (Ours)\nRPN (Faster R\u2212CNN)\nFigure 10. Comparison of recall of region proposals generat ed by\nAZ-Net and RPN at different number of region proposals on VOC\n2007 test. The comparison is performed at IoU threshold 0.5. Our\napproach has better early recall. In particular, it reaches 0.6recall\nwith only 10 proposals.Small Medium Large00.20.40.60.81\nSize of Ground Truth BoxesRecall at IoU Threshold = 0.50\n  \nAZ\u2212Net (Ours)\nRPN (Faster R\u2212CNN)\nFigure 11. Comparison of recall of region proposals generat ed by\nAZ-Net and RPN for objects of different sizes on VOC 2007 test .\nThe comparison is performed at IoU threshold 0.5with top-300\nproposals. Our approach has signi\ufb01cantly better recall for small\nobjects.\nMethod Anchor Regions Region proposals Runtime (ms)\nAZ-Net 62 231 171\nAZ-Net* 44 228 237\nRPN 2400 300 198\nRPN* 2400 300 342\nFRCNN N/A 2000 1830\nTable 2. Numbers related to the ef\ufb01ciency of the object detec tion",
      "start_page": 0,
      "end_page": 0
    },
    {
      "title": "methods listed in Table 1. The runtimes for RPN and Fast R-CNN",
      "content": "are reported for a K40 GPU [22]. Our runtime experiment is per -\nformed on a GTX 980Ti GPU. The K40 GPU has larger GPU\nmemory, while the GTX 980Ti has higher clock rate. * indicate s\nunshared convolutional feature version.\nthresholds. Our AZ-net has consistently higher recall than\nRPN, and the advantage is larger at higher IoU thresholds.\nThis suggests our method generates bounding boxes that in\ngeneral overlap with the ground truth objects better. The\nproposals are also more concentrated around objects, as\nshown in Figure 9.\nFigure 10 shows a plot of recall as a function of the num-\nber of proposals. A region proposal algorithm is more ef\ufb01-\ncient in covering objects if its area under the curve is large r.\nOur experiment suggests that our AZ-Net approach has a\nbetter early recall than RPN. That means our algorithm in\ngeneral can recover more objects with the same number of\nregion proposals.\nFigure 11 shows a comparison of recall for objects with\ndifferent sizes. The \u201csmall object\u201d has an area less than\n322, a \u201cmedium object\u201d has an area between 322and962,\nand a \u201clarge object\u201d has an area greater than 962, same as\nthe de\ufb01nition in MSCOCO [18]. Our approach achieves\nhigher recall on the small object subset. This is because\nwhen small objects are present in the scene our adaptive\nsearch strategy generates small anchor regions around them ,\nas shown in Figure 7.\n4.3. Ef\ufb01ciency of Adaptive Search\nOur approach is ef\ufb01cient in runtime, as shown in Table\n2. We note that this is achieved even with several severe in-\n2357\n\n0 50 100 150 200 250 3000100200300400500600700\nNumber of Evaluated Anchor RegionsFrequency\nMean: 62Median: 40\n96% of the\ndistribution\nFigure 12. Distribution of the number of anchor regions eval uated\non VOC 2007 test set. For most images a few dozen anchor region s\nare required. Note that anchors are shared across categorie s.\nef\ufb01ciencies in our implementation. First, for each image\nour algorithm requires several rounds of fully connected\nlayer evaluation, which induces expensive memory trans-\nfer between GPU and CPU. Secondly, the Faster R-CNN\napproach uses convolutional computation for the evaluatio n\nof anchor regions, which is highly optimized compared to\nthe RoI pooling technique we adopted. Despite these in-\nef\ufb01ciencies, our approach still achieves high accuracy at a\nstate-of-the-art frame rate, using lower-end hardware. Wi th\nimproved implementation and model design we expect our\nalgorithm to be signi\ufb01cantly faster.\nAn interesting aspect that highlights the advantages of\nour approach is the small number of anchor regions to eval-\nuate. To further understand this aspect of our algorithm,\nwe show in Figure 12 the distribution of anchor regions\nevaluated for each image. For most images our method\nonly requires a few dozen anchor regions. This number is\nmuch smaller than the 2400 anchor regions used in RPN\n[22] and the 800 used in MultiBox [5]. Future work could\nfurther capitalize on this advantage by using an expensive\nbut more accurate per-anchor step, or by exploring applica-\ntions to very high-resolution images, for which traditiona l\nnon-adaptive approaches will face intrinsic dif\ufb01culties d ue\nto scalability issues. Our experiment also demonstrates th e\npossibility of designing a class-generic search. Unlike pe r-\nclass search methods widely used in previous adaptive ob-\nject detection schemes [3, 28] our anchor regions are shared\namong object classes, making it ef\ufb01cient for multi-class de -\ntection.\n4.4. Results on MSCOCO\nWe also evaluated our method on MSCOCO dataset and\nsubmitted a \u201cUCSD\u201d entry to the MSCOCO 2015 detection\nchallenge. Our post-competition work greatly improved ac-\ncuracy with more training iterations. A comparison with\nother recent methods is shown in Table 3. Our model isMethod AP AP IoU=0.50\nFRCNN (VGG16) [9] 19.7 35.9\nFRCNN (VGG16) [22] 19.3 39.3\nRPN (VGG16) 21.9 42.7\nRPN (ResNet) 37.4 59.0\nAZ-Net (VGG16) 22.3 41.0\nTable 3. The detection mAP on MSCOCO 2015 test-dev set. The\nRPN (ResNet) entry won the MSCOCO 2015 detection challenge.\nUpdated leaderboard can be found in http://mscoco.org .\ntrained with minibatches consisting of 256 regions sampled\nfrom one image, and 720k iterations in total. The results\nfor RPN(VGG16) reported in [22] were obtained with an\n8-GPU implementation that effectively has 8 and 16 images\nper minibatch for RPN and Fast R-CNN respectively, each\ntrained at 320k training iterations. Despite the much short er\neffective training iterations, our AZ-Net achieves simila r\nmAP with RPN(VGG16) and is more accurate when eval-\nuated on the MSCOCO mAP metric that rewards accurate\nlocalization.\nOur best post-competition model is still signi\ufb01cantly\noutperformed by the winning \u201cMSRA\u201d entry. Their ap-\nproach is a Faster-R-CNN-style detection pipeline, replac -\ning the VGG-16 network with an ultra-deep architecture\ncalled Deep Residual Network [13]. They also report signif-\nicant improvement from using model ensembles and global\ncontextual information. We note that these developments\nare complementary to our contribution.\n5. Conclusion and Future Work\nThis paper has introduced an adaptive object detection\nsystem using adjacency and zoom predictions. Our al-\ngorithm adaptively focuses its computational resources on\nsmall regions likely to contain objects, and demonstrates\nstate-of-the-art accuracy at a fast frame rate.\nThe current method can be further extended and im-\nproved in many aspects. Better pre-trained models [13]\ncan be incorporated into the current system for even bet-\nter accuracy. Further re\ufb01ning the model to allow single-\npipeline detection that directly predicts class labels, as in\nYOLO [21] and the more recent SSD [19] method, could\nsigni\ufb01cantly boost testing frame rate. Recent techniques\nthat improve small object detection, such as the contextual\nmodel and skip layers adopted in Inside-Outside Net [1],\nsuggest additional promising directions. It is also intere st-\ning to consider more aggressive extensions. For instance, i t\nmight be advantageous to use our search structure to focus\nhigh-resolution convolutional layer computation on small er\nregions, especially for very high-resolution images.\nAcknowledgment\nThis work is supported by the National Science Foun-\ndation grants CIF-1302438, CCF-1302588 and CNS-\n1329819, as well as Xerox UAC, and the Sloan Foundation.\n2358\n\nReferences\n[1] S. Bell, C. L. Zitnick, K. Bala, and R. Girshick. Inside-\noutside net: Detecting objects in context with skip\npooling and recurrent neural networks. arXiv preprint\narXiv:1512.04143 , 2015.\n[2] A. Borji and L. Itti. State-of-the-art in visual attenti on model-\ning.Pattern Analysis and Machine Intelligence, IEEE Trans-\nactions on , 35(1):185\u2013207, 2013.\n[3] J. C. Caicedo and S. Lazebnik. Active object localizatio n\nwith deep reinforcement learning. In The IEEE International\nConference on Computer Vision (ICCV) , December 2015.\n[4] S. K. Divvala, D. Hoiem, J. H. Hays, A. Efros, M. Hebert,\net al. An empirical study of context in object detection.\nInComputer Vision and Pattern Recognition, 2009. CVPR\n2009. IEEE Conference on , pages 1271\u20131278. IEEE, 2009.\n[5] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalabl e\nobject detection using deep neural networks. In Computer\nVision and Pattern Recognition (CVPR), 2014 IEEE Confer-\nence on , pages 2155\u20132162. IEEE, 2014.\n[6] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\nand A. Zisserman. The PASCAL Visual Object Classes\nChallenge 2007 (VOC2007) Results. http://www.pascal-\nnetwork.org/challenges/VOC/voc2007/workshop/index.h tml.\n[7] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\nand A. Zisserman. The PASCAL Visual Object Classes\nChallenge 2012 (VOC2012) Results. http://www.pascal-\nnetwork.org/challenges/VOC/voc2012/workshop/index.h tml.\n[8] S. Gidaris and N. Komodakis. Object detection via a multi -\nregion and semantic segmentation-aware cnn model. In The\nIEEE International Conference on Computer Vision (ICCV) ,\nDecember 2015.\n[9] R. Girshick. Fast r-cnn. In The IEEE International Confer-\nence on Computer Vision (ICCV) , December 2015.\n[10] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich f ea-\nture hierarchies for accurate object detection and semanti c\nsegmentation. In Computer Vision and Pattern Recognition\n(CVPR), 2014 IEEE Conference on , pages 580\u2013587. IEEE,\n2014.\n[11] A. Gonzalez-Garcia, A. Vezhnevets, and V . Ferrari. An a c-\ntive search strategy for ef\ufb01cient object class detection. I n\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , pages 3022\u20133031, 2015.\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pool-\ning in deep convolutional networks for visual recognition.\nInComputer Vision\u2013ECCV 2014 , pages 346\u2013361. Springer,\n2014.\n[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-\ning for image recognition. arXiv preprint arXiv:1512.03385 ,\n2015.\n[14] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\nshick, S. Guadarrama, and T. Darrell. Caffe: Convolu-\ntional architecture for fast feature embedding. arXiv preprint\narXiv:1408.5093 , 2014.\n[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\nclassi\ufb01cation with deep convolutional neural networks. In\nAdvances in neural information processing systems , pages\n1097\u20131105, 2012.[16] C. H. Lampert, M. B. Blaschko, and T. Hofmann. Ef\ufb01cient\nsubwindow search: A branch and bound framework for ob-\nject localization. Pattern Analysis and Machine Intelligence,\nIEEE Transactions on , 31(12):2129\u20132142, 2009.\n[17] X. Liang, S. Liu, Y . Wei, L. Liu, L. Lin, and S. Yan. To-\nwards computational baby learning: A weakly-supervised\napproach for object detection. In The IEEE International\nConference on Computer Vision (ICCV) , December 2015.\n[18] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. R a-\nmanan, P. Doll\u00b4 ar, and C. L. Zitnick. Microsoft coco: Com-\nmon objects in context. In Computer Vision\u2013ECCV 2014 ,\npages 740\u2013755. Springer, 2014.\n[19] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed.\nSsd: Single shot multibox detector. arXiv preprint\narXiv:1512.02325 , 2015.\n[20] Y . Lu and T. Javidi. Ef\ufb01cient object detection for high r eso-\nlution images. arXiv preprint arXiv:1510.01257 , 2015.\n[21] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You\nonly look once: Uni\ufb01ed, real-time object detection. arXiv\npreprint arXiv:1506.02640 , 2015.\n[22] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towa rds\nreal-time object detection with region proposal networks. In\nC. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and\nR. Garnett, editors, Advances in Neural Information Process-\ning Systems 28 , pages 91\u201399. Curran Associates, Inc., 2015.\n[23] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556 , 2014.\n[24] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed,\nD. Anguelov, D. Erhan, V . Vanhoucke, and A. Rabi-\nnovich. Going deeper with convolutions. arXiv preprint\narXiv:1409.4842 , 2014.\n[25] C. Szegedy, A. Toshev, and D. Erhan. Deep neural network s\nfor object detection. In Advances in Neural Information Pro-\ncessing Systems , pages 2553\u20132561, 2013.\n[26] A. Torralba, A. Oliva, M. S. Castelhano, and J. M. Hender -\nson. Contextual guidance of eye movements and attention\nin real-world scenes: the role of global features in object\nsearch. Psychological review , 113(4):766, 2006.\n[27] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W.\nSmeulders. Selective search for object recognition. Interna-\ntional journal of computer vision , 104(2):154\u2013171, 2013.\n[28] D. Yoo, S. Park, J.-Y . Lee, A. S. Paek, and I. So Kweon. At-\ntentionnet: Aggregating weak directions for accurate obje ct\ndetection. In The IEEE International Conference on Com-\nputer Vision (ICCV) , December 2015.\n[29] C. L. Zitnick and P. Doll\u00b4 ar. Edge boxes: Locating objec t pro-\nposals from edges. In Computer Vision\u2013ECCV 2014 , pages\n391\u2013405. Springer, 2014.\n2359\n\n",
      "start_page": 0,
      "end_page": 0
    }
  ],
  "formulas": [],
  "full_text": "Adaptive Object Detection Using Adjacency and Zoom Predict ion\nYongxi Lu\nUniversity of California, San Diego\nyol070@ucsd.eduTara Javidi\nUniversity of California, San Diego\ntjavidi@ucsd.edu\nSvetlana Lazebnik\nUniversity of Illinois at Urbana-Champaign\nslazebni@illinois.edu\nAbstract\nState-of-the-art object detection systems rely on an ac-\ncurate set of region proposals. Several recent methods use\na neural network architecture to hypothesize promising ob-\nject locations. While these approaches are computation-\nally ef\ufb01cient, they rely on \ufb01xed image regions as anchors\nfor predictions. In this paper we propose to use a search\nstrategy that adaptively directs computational resources to\nsub-regions likely to contain objects. Compared to meth-\nods based on \ufb01xed anchor locations, our approach natu-\nrally adapts to cases where object instances are sparse and\nsmall. Our approach is comparable in terms of accuracy\nto the state-of-the-art Faster R-CNN approach while using\ntwo orders of magnitude fewer anchors on average. Code is\npublicly available.\n1. Introduction\nObject detection is an important computer vision prob-\nlem for its intriguing challenges and large variety of ap-\nplications. Signi\ufb01cant recent progress in this area has\nbeen achieved by incorporating deep convolutional neu-\nral networks (DCNN) [15] into object detection systems\n[5, 8, 10, 12, 17, 23, 25].\nAn object detection algorithm with state-of-the-art ac-\ncuracy typically has the following two-step cascade: a set\nof class-independent region proposals are hypothesized an d\nare then used as input to a detector that gives each region\na class label. The role of region proposals is to reduce the\ncomplexity through limiting the number of regions that need\nbe evaluated by the detector. However, with recently intro-\nduced techniques that enable sharing of convolutional fea-\ntures [9, 12], traditional region proposal algorithms such as\nselective search [27] and EdgeBoxes [29] become the bot-\ntleneck of the detection pipeline.\nAn emerging class of ef\ufb01cient region proposal meth-ods are based on end-to-end trained deep neural networks\n[5, 22]. The common idea in these approaches is to train\na class-independent regressor on a small set of pre-de\ufb01ned\nanchor regions. More speci\ufb01cally, each anchor region is as-\nsigned the task of deciding whether an object is in its neigh-\nborhood (in terms of center location, scale and aspect ratio ),\nand predicting a bounding box for that object through re-\ngression if that is the case. The design of anchors differs fo r\neach method. For example, MultiBox [5] uses 800 anchors\nfrom clustering, YOLO [21] uses a non-overlapping 7 by 7\ngrid, RPN [22] uses overlapping sliding windows. In these\nprior works the test-time anchors are not adaptive to the ac-\ntual content of the images, thus to further improve accuracy\nfor detecting small object instances a denser grid of anchor s\nis required for all images, resulting in longer test time and a\nmore complex network model.\nWe alternatively consider the following adaptive search\nstrategy. Instead of \ufb01xing a priori a set of anchor regions,\nour algorithm starts with the entire image. It then recur-\nsively divides the image into sub-regions (see Figure 2) un-\ntil it decides that a given region is unlikely to enclose any\nsmall objects. The regions that are visited in the process\neffectively serve as anchors that are assigned the task of\npredicting bounding boxes for objects nearby. A salient\nfeature of our algorithm is that the decision of whether to\ndivide a region further is based on features extracted from\nthat particular region. As a result, the generation of the se t\nof anchor regions is conditioned on the image content. For\nan image with only a few small objects most regions are\npruned early in the search, leaving a few small anchor re-\ngions near the objects. For images that contain exclusively\nlarge instances, our approach gracefully falls back to exis t-\ning methods that rely on a small number of large anchor\nregions. In this manner, our algorithm adaptively directs i ts\ncomputational resources to regions that are likely to conta in\nobjects. Figure 1 compares our algorithm with RPN.\nTo support our adaptive search algorithm, we train a deep\n2351\n\nFigure 1. Comparison of our proposed adaptive search algori thm with the non-adaptive RPN method. The red boxes show regi on proposals\nfrom adjacency predictions. Note that for small objects, RP N is forced to perform regression from much larger anchors, w hile our AZ-Net\napproach can adaptively use features from small regions.\nneural network we call Adjacency and Zoom Network (AZ-\nNet). Given an input anchor region, the AZ-Net outputs a\nscalar zoom indicator which is used to decide whether to\nfurther zoom into (divide) the region and a set of bound-\ning boxes with con\ufb01dence scores, or adjacency predictions .\nThe adjacency predictions with high con\ufb01dence scores are\nthen used as region proposals for a subsequent object de-\ntector. The network is applied recursively starting from th e\nwhole image to generate an adaptive set of proposals.\nTo intuitively motivate the design of our network, con-\nsider a situation in which one needs to perform a quick\nsearch for a car. A good strategy is to \ufb01rst look for\nlarger structures that could provide evidence for existenc e\nof smaller structures in related categories. A search agent\ncould, for example, look for roads and use that to reason\nabout where cars should be. Once the search nears the car,\none could use the fact that seeing certain parts is highly pre -\ndictive of the spatial support of the whole. For instance,\nthe wheels provide strong evidence for a tight box of the\ncar. In our design, the zoom indicator mimics the process\nof searching for larger structures, while the adjacency pre -\ndictions mimic the process of neighborhood inference.\nTo validate this design we extensively evaluate our al-\ngorithm on Pascal VOC 2007 [6] with \ufb01ne-grained analy-\nsis. We also report baseline results on the recently intro-\nduced MSCOCO [18] dataset. Our algorithm achieves de-\ntection mAP that is close to state-of-the-art methods at a\nfast frame rate. Code has been made publicly available at\nhttps://github.com/luyongxi/az-net .\nIn summary, we make the following contributions:\n\u2022We design a search strategy for object detection that\nadaptively focuses computational resources on image\nregions that contain objects.\n\u2022We evaluate our approach on Pascal VOC 2007 and\nMSCOCO datasets and demonstrate it is comparableto Fast R-CNN and Faster R-CNN with fewer anchor\nand proposal regions.\n\u2022We provide a \ufb01ne-grained analysis that shows intrigu-\ning features of our approach. Namely, our proposal\nstrategy has better recall for higher intersection-over-\nunion thresholds, higher recall for smaller numbers of\ntop proposals, and for smaller object instances.\nThis paper is organized as follows. In section 2 we sur-\nvey existing literature highlighting the novelty of our ap-\nproach. In Section 3 we introduce the design of our algo-\nrithm. Section 4 presents an empirical comparison to exist-\ning object detection methods on standard evaluation bench-\nmarks, and Section 5 discusses possible future directions.\n2. Previous Work\nLampert et al . [16] \ufb01rst proposed an adaptive branch-\nand-bound approach. More recently, Gonzeles-Garcia et al.\n[11], Caicedo and Lazebnik [3], and Yoo et al. [28] explored\nactive object detection with DCNN features. While these\napproaches show the promise of using an adaptive algo-\nrithm for object detection, their detectors are class-wise and\ntheir methods cannot achieve competitive accuracy. Our ap-\nproach, on the other hand, is multi-class and is compara-\nble to state-of-the-art approaches in both accuracy and tes t\nspeed.\nThe idea of using spatial context has been previously ex-\nplored in the literature. Previous work by Torralba et al .\n[26] used a biologically inspired visual attention model [2 ],\nbut our focus is on ef\ufb01cient engineering design. Divvala et\nal. [4] evaluated the use of context for localization, but thei r\nempirical study was performed on hand-crafted features and\nneeds to be reexamined in combination with more accurate\nrecent approaches.\nOur method is closely related to recent approaches that\n2352\n\n1 2\n345\nFigure 2. As illustrated, a given region is divided into 5 sub -regions\n(numbered). Each of these sub-regions is recursively divid ed if its\nzoom indicator is above a threshold.\nNo zoom No zoom Zoom\nFigure 3. Illustration of desired zoom indicator for common situa-\ntions. The green boxes are objects, and the red boxes are regi ons.\nLeft: the object is small but it is mostly outside the region \u2013 there\nis no gain in zooming in. Middle: the object is mostly inside b ut\nits size is large relative to the region \u2013 there is no gain in zo oming\nin. Right: there is a small object that is completely inside t he re-\ngion. In this case further division of the region greatly inc reases\nthe chance of detection for that object.\nuse anchor regions for proposal generation or detection. Fo r\nexample, Erhan et al. [5] use 800 data-driven anchors for re-\ngion proposals and Redmon et al. [21] use a \ufb01xed grid of 49\nnon-overlapping regions to provide class-wise detections .\nThe former has the concern that these anchors could over-\n\ufb01t the data, while the latter cannot achieve state-of-the-a rt\nperformance without model ensembles. Our work is most\nrelated to the recent work by Ren et al. [22], which uses a set\nof heuristically designed 2400 overlapping anchor regions .\nOur approach uses a similar regression technique to predict\nmultiple bounding boxes from an anchor region. However,\nour anchor regions are generated adaptively, making them\nintrinsically more ef\ufb01cient. In particular, we show that it is\npossible to detect small object instances in the scene with-\nout an excessive number of anchor regions. We propose to\ngrow a tree of \ufb01ner-grained anchor regions based on local\nimage evidence, and design the regression model strategi-\ncally on top of it. We extensively compare the output of our\nmethod against [22] in our experimental section and show\nthe unique advantages of our approach.\nThis paper is a follow-up to the work published in the\n53rd Annual Allerton Conference [20]. Here, we introduce\na substantially improved algorithm and add extensive eval-\nuations on standard benchmarks.1 231\n23\n12\n3\n4\nFigure 4. Illustration of sub-region priors. From left to ri ght: ver-\ntical stripes, horizontal stripes, neighboring squares. T he red rect-\nangular box is the image. In the \ufb01gure the numbered regions ar e\ntemplate sub-regions. The gaps between sub-regions are exa gger-\nated for better visualization. The vertical stripes are use d to detect\ntall objects, the horizontal stripes are used to detect fat o bjects,\nwhile the neighboring squares are used to detect objects tha t fall\nin the gaps between anchor regions generated in the search pr o-\ncess.\n3. Design of the Algorithm\n3.1. Overview of the Adaptive Search\nOur object detection algorithm consists of two steps. In\nstep 1, a set of class-independent region proposals are gen-\nerated using Adaptive Search with AZ-Net (see Algorithm\n1). In step 2, an object detector evaluates each region pro-\nposed in step 1 to provide class-wise detections. In our ex-\nperiments the detector is Fast R-CNN.\nOur focus is on improving step 1. We consider a recur-\nsive search strategy, starting from the entire image as the\nroot region. For any region encountered in the search pro-\ncedure, the algorithm extracts features from this region to\ncompute the zoom indicator and the adjacency predictions.\nThe adjacency predictions with con\ufb01dence scores above a\nthreshold are included in the set of output region proposals .\nIf the zoom indicator is above a threshold, this indicates\nthat the current region is likely to contain small objects. T o\ndetect these embedded small objects, the current region is\ndivided into sub-regions in the manner shown in Figure 2.\nEach of these sub-regions is then recursively processed in\nthe same manner as its parent region, until either its area\nor its zoom indicator is too small. Figure 1 illustrates this\nprocedure.\nIn the following section, we discuss the design of the\nzoom indicator and adjacency prediction.\n3.2. Design of Building Blocks\nThe zoom indicator should be large for a region only\nwhen there exists at least one object whose spatial support\nmostly lies within the region, and whose size is suf\ufb01ciently\nsmall compared to the region. The reasoning is that we\nshould zoom in to a region only when it substantially in-\ncreases the chance of detection. For example, if an object is\nmostly outside the region, dividing the region further is un -\nlikely to increase the chance of detecting that object. Sim-\nilarly, if an object is large compared to the current region,\nthe task of detecting this object should be handled by this\n2353\n\nAlgorithm 1: Adaptive search with AZ-Net.\nData : Input image x(the whole image region bx).Ykis the region proposed at step k.Ykare the accumulated region\nproposals up to step k.Zkare the regions to further zoom in to at step k.Bkare anchor regions at step k.\nResult : Region proposals at termination YK.\nInitialization: B0\u2190{bx}.Y0\u2190\u2205,k\u21900\nwhile (Bkis not an empty set) do\nInitialize YkandZkas empty sets.\nforeachb\u2208Bkdo\nCompute adjacency predictions Aband the zoom indicator zbusing AZ-Net.\nInclude all a\u2208Abwith high con\ufb01dence scores into Yk.\nIncludebintoZkifzbis above threshold.\nend\nYk\u2190Yk\u22121\u222aYk\nBk+1\u2190Divide-Regions (Zk)\nk\u2190k+1\nend\nK\u2190k\u22121\nFigure 5. Illustration of the AZ-Net architecture.\nregion or its parents. In the latter case, further division o f\nthe region not only wastes computational resources, but als o\nintroduces false positives in the region proposals. Figure 3\nshows common situations and the desirable behavior of the\nzoom indicator.\nThe role of adjacency prediction is to detect one or mul-\ntiple objects that overlap with the anchor region suf\ufb01cient ly\nby providing tight bounding boxes. The adjacency predic-\ntion should be aware of the search geometry induced by the\nzoom indicator. More speci\ufb01cally, the adjacency predic-\ntion should perform well on the effective anchor regions in-\nduced by the search algorithm. For this purpose we propose\na training procedure that is aware of the adaptive search\nscheme (discussed in Section 3.3). On the other hand, its\ndesign should explicitly account for typical geometric con -\n\ufb01gurations of objects that fall inside the region, so that th e\ntraining can be performed in a consistent fashion. For this\nreason, we propose to make predictions based on a set of\nsub-region priors as shown in Figure 4. Note that we also\ninclude the anchor region itself as an additional prior. Wemake sub-region priors large compared to the anchor under\nthe intuition that if an object is small, it is best to wait unt il\nthe features extracted are at the right scale to make bound-\ning box predictions.\n3.3. Implementation\nWe implement our algorithm using the Caffe [14] frame-\nwork, utilizing the open source infrastructure provided by\nthe Fast R-CNN repository [9]. In this section we intro-\nduce the implementation details of our approach. We use\nthe Fast R-CNN detector since it is a fast and accurate re-\ncent approach. Our method should in principle work for a\nbroad class of object detectors that use region proposals.\nWe train a deep neural network as illustrated in Figure\n5. Note that in addition to the sub-region priors as shown\nin Figure 4, we also add the region itself as a special prior\nregion, making in total 11adjacency predictions per anchor.\nFor the convolutional layers, we use the VGG16 model [23]\npre-trained on ImageNet data. The fully-connected layers\nare on top of a region pooling layer introduced in [9] which\nallows ef\ufb01cient sharing of convolutional layer features.\nThe training is performed as a three-step procedure.\nFirst, a set of regions is sampled from the image. These\nsamples should contain hard positive and negative exam-\nples for both the zoom indicator and the adjacency predic-\ntion. Finally, the tuples of samples and labels are used in\nstandard stochastic gradient descent training. We now dis-\ncuss how the regions are sampled and labeled, and the loss\nfunction we choose.\n3.3.1 Region Sampling and Labeling\nSince a typical image only has a few object instances, to\nprovide suf\ufb01cient positive examples for adjacency predic-\n2354\n\nFigure 6. Illustration of the inverse matching procedure. T he red\nbox is the inverse match for the object (green box). The left \ufb01 gure\nshows inverse matching of a neighboring square, the right \ufb01g ure\nshows inverse matching of a vertical stripe.\ntions our method inversely \ufb01nds regions that will see a\nground truth object as a perfect \ufb01t to its prior sub-regions\n(see Figure 6 for illustration). This provides k\u00d711training\nexamples for each image, where kis the number of objects.\nTo mine for negative examples and hard positive exam-\nples, we search the input image as in Algorithm 1. Note\nthat the algorithm uses zoom indicators from the AZ-Net.\nInstead of optimizing AZ-Net with an on-policy approach\n(that uses the intermediate AZ-Net model to sample re-\ngions), which might cause training to diverge, we replace\nthe zoom prediction with the zoom indicator label. How-\never, we note that using the zoom label directly could cause\nover\ufb01tting, since at test time the algorithm might encounte r\nsituations where a previous zoom prediction is wrong. To\nimprove the robustness of the model, we add noise to the\nzoom label by \ufb02ipping the ground truth with a probability\nof0.3. We found that models trained without random \ufb02ip-\nping are signi\ufb01cantly less accurate. For each input image\nwe initiate this procedure with \ufb01ve sub-images and repeat it\nmultiple times. We also append horizontally \ufb02ipped images\nto the dataset for data augmentation.\nAssignment of labels for the zoom indicator follows the\ndiscussion of Section 3. The label is 1 if there exists an ob-\nject with 50% of its area inside the region and the area is\nat most 25% of the size of the region. Note that here we\nuse a loose de\ufb01nition of inclusion to add robustness for ob-\njects falling between boundaries of anchors. For adjacency\nprediction, we set a threshold in the intersection-over-un ion\n(IoU) score between an object and a region. A region is\nassigned to detect objects with which it has suf\ufb01cient over-\nlap. The assigned objects are then greedily matched to one\nof the sub-regions de\ufb01ned by the priors shown in Figure 4.\nThe priority in the matching is determined by the IoU score\nbetween the objects and the sub-regions. We note that in\nthis manner multiple predictions from a region are possible .\n3.3.2 Loss Function\nAs shown in Figure 5, the AZ-Net has three output layers.\nThe zoom indicator outputs from a sigmoid activation func-\ntion. To train it we use the cross-entropy loss function pop-\nular for binary classi\ufb01cation. For the adjacency predictio ns,\nthe bounding boxes are parameterized as in Fast R-CNN[22]. Unlike in Fast R-CNN, to provide multiple predic-\ntions from any region, the con\ufb01dence scores are not nor-\nmalized to a probability vector. Correspondingly we use\nsmooth L1-loss for bounding box output and element-wise\ncross-entropy loss for con\ufb01dence score output. The three\nlosses are summed together to form a multi-task loss func-\ntion.\n3.3.3 Fast R-CNN Detectors\nThe detectors we use to evaluate proposal regions are Fast\nR-CNN detectors trained using AZ-Net proposals. As in\n[22], we implement two versions: one with unshared con-\nvolutional features and the other that shares convolutiona l\nfeatures with AZ-Net. The shared version is trained using\nalternating optimization.\n4. Experiments\nWe evaluate our approach on Pascal VOC 2007 [6] and\nMSCOCO [18] datasets. In addition to evaluating the accu-\nracy of the \ufb01nal detectors, we also perform detailed compar-\nisons between the RPN approach adopted in Faster R-CNN\nand our AZ-Net on VOC 2007. At the end of the section,\nwe give an analysis of the ef\ufb01ciency of our adaptive search\nstrategy.\n4.1. Results on VOC 2007\nTo set up a baseline comparison, we evaluate our ap-\nproach using the standard average precision (AP) metric\nfor object detection. For AP evaluation we use the de-\nvelopment kit provided by the VOC 2007 object detection\nchallenge. We compare our approach against the recently\nintroduced Fast R-CNN [9] and Faster R-CNN [22] sys-\ntems, which achieve state-of-the-art performance in stan-\ndard benchmarks, such as VOC 2007 [6] and VOC 2012\n[7]. A comparison is shown in Table 1. The results sug-\ngest that our approach is comparable to or better than these\nmethods.\n4.2. Quality of Region Proposals\nWe preform a detailed analysis of the quality of region\nproposals from our AZ-Net, highlighting a comparison to\nthe RPN network used in Faster R-CNN. For all our ex-\nperiments, we analyze the recall on Pascal VOC 2007 test\nset using the following de\ufb01nition: An object is counted as\nretrieved if there exists a region proposal with an above-\nthreshold IoU with it. The recall is then calculated as the\nproportion of the retrieved objects among all ground truth\nobject instances. To accurately reproduce the RPN ap-\nproach, we downloaded the region proposals provided on\nthe Faster R-CNN repository1. We used the results from\n1https://github.com/ShaoqingRen/faster_rcnn\n2355\n\nFigure 7. Example outputs of our algorithm. The left column s hows the original image. The middle column shows the anchor r egions\ninduced by our adaptive search. The right column shows the to p 100 adjacency predictions made around the anchor regions. The anchor\nregions and the adjacency predictions are superimposed int o a \ufb01gure at the same resolution of the original image. We note that the anchor\nregions and the region proposals in our approach are shared a cross object categories. For example, for the last image, th e algorithm\ngenerates anchor regions at proper scales near the dogs, the person, and the bottles.\nMethod boxes mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv\nAZ-Net 231 70.2 73.3 78.8 69.2 59.9 48.7 81.4 82.8 83.6 47.5 77.3 62.9 81.1 83.5 78.0 75.8 38.0 68.7 67.2 79.0 66.4\nAZ-Net* 228 70.4 73.9 79.9 68.8 58.9 49.1 80.8 83.3 83.7 47.2 75.8 63.8 80.6 84.4 78.9 75.8 39.2 70.2 67.4 78.4 68.3\nRPN 300 69.9 70.0 80.6 70.1 57.3 49.9 78.2 80.4 82.0 52.2 75.3 67.2 80.3 79.8 75.0 76.3 39.1 68.3 67.3 81.1 67.6\nRPN* 300 68.5 74.1 77.2 67.7 53.9 51.0 75.1 79.2 78.9 50.7 78.0 61.1 79.1 81.9 72.2 75.9 37.2 71.4 62.5 77.4 66.4\nFRCNN 2000 68.1 74.6 79.0 68.6 57.0 39.3 79.5 78.6 81.9 48.0 74.0 67.4 80.5 80.7 74.1 69.6 31.8 67.1 68.4 75.3 65.5\nTable 1. Comparison on VOC 2007 test set using VGG-16 for conv olutional layers. The results of RPN are reported in [22]. Th e results\nfor Fast R-CNN are reported in [9]. The AZ-Net and RPN results are reported for top-300 region proposals, but in AZ-Net man y images\nhave too few anchors to generate 300 proposals. * indicates r esults without shared convolutional features. All listed m ethods use DCNN\nmodels trained on VOC 2007 trainval.\na model reportedly trained on VOC 2007 trainval. Cor-\nrespondingly we compare it against our model trained on\nVOC 2007 trainval set. The comparisons concerning top-N regions are performed by ranking the region proposals in\norder of their con\ufb01dence scores.\nFigure 8 shows a comparison of recall at different IoU\n2356\n\n0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.9500.20.40.60.81\nThreshold at IoURecall at Top\u2212300 Region Proposals AZ\u2212Net (Ours)\nRPN (Faster R\u2212CNN)\nFigure 8. Comparison of recall of region proposals generate d by\nAZ-Net and RPN at different intersection over union thresho lds\non VOC 2007 test. The comparison is performed at top-300 re-\ngion proposals. Our approach has better recall at large IoU t hresh-\nolds, which suggests that AZ-Net proposals are more accurat e in\nlocalizing the objects.\nSmall Medium Large05101520\nSize of Ground Truth BoxesNumber of Matching Proposals AZ\u2212Net (Ours)\nRPN (Faster R\u2212CNN)\nFigure 9. Number of proposals matched to ground truth (with\nIoU= 0.5). This shows proposals from AZ-Net are more con-\ncentrated around true object locations.\n10010110200.20.40.60.81\nNumber of Region ProposalsRecall at IoU Threshold = 0.50 \nAZ\u2212Net (Ours)\nRPN (Faster R\u2212CNN)\nFigure 10. Comparison of recall of region proposals generat ed by\nAZ-Net and RPN at different number of region proposals on VOC\n2007 test. The comparison is performed at IoU threshold 0.5. Our\napproach has better early recall. In particular, it reaches 0.6recall\nwith only 10 proposals.Small Medium Large00.20.40.60.81\nSize of Ground Truth BoxesRecall at IoU Threshold = 0.50\n  \nAZ\u2212Net (Ours)\nRPN (Faster R\u2212CNN)\nFigure 11. Comparison of recall of region proposals generat ed by\nAZ-Net and RPN for objects of different sizes on VOC 2007 test .\nThe comparison is performed at IoU threshold 0.5with top-300\nproposals. Our approach has signi\ufb01cantly better recall for small\nobjects.\nMethod Anchor Regions Region proposals Runtime (ms)\nAZ-Net 62 231 171\nAZ-Net* 44 228 237\nRPN 2400 300 198\nRPN* 2400 300 342\nFRCNN N/A 2000 1830\nTable 2. Numbers related to the ef\ufb01ciency of the object detec tion\nmethods listed in Table 1. The runtimes for RPN and Fast R-CNN\nare reported for a K40 GPU [22]. Our runtime experiment is per -\nformed on a GTX 980Ti GPU. The K40 GPU has larger GPU\nmemory, while the GTX 980Ti has higher clock rate. * indicate s\nunshared convolutional feature version.\nthresholds. Our AZ-net has consistently higher recall than\nRPN, and the advantage is larger at higher IoU thresholds.\nThis suggests our method generates bounding boxes that in\ngeneral overlap with the ground truth objects better. The\nproposals are also more concentrated around objects, as\nshown in Figure 9.\nFigure 10 shows a plot of recall as a function of the num-\nber of proposals. A region proposal algorithm is more ef\ufb01-\ncient in covering objects if its area under the curve is large r.\nOur experiment suggests that our AZ-Net approach has a\nbetter early recall than RPN. That means our algorithm in\ngeneral can recover more objects with the same number of\nregion proposals.\nFigure 11 shows a comparison of recall for objects with\ndifferent sizes. The \u201csmall object\u201d has an area less than\n322, a \u201cmedium object\u201d has an area between 322and962,\nand a \u201clarge object\u201d has an area greater than 962, same as\nthe de\ufb01nition in MSCOCO [18]. Our approach achieves\nhigher recall on the small object subset. This is because\nwhen small objects are present in the scene our adaptive\nsearch strategy generates small anchor regions around them ,\nas shown in Figure 7.\n4.3. Ef\ufb01ciency of Adaptive Search\nOur approach is ef\ufb01cient in runtime, as shown in Table\n2. We note that this is achieved even with several severe in-\n2357\n\n0 50 100 150 200 250 3000100200300400500600700\nNumber of Evaluated Anchor RegionsFrequency\nMean: 62Median: 40\n96% of the\ndistribution\nFigure 12. Distribution of the number of anchor regions eval uated\non VOC 2007 test set. For most images a few dozen anchor region s\nare required. Note that anchors are shared across categorie s.\nef\ufb01ciencies in our implementation. First, for each image\nour algorithm requires several rounds of fully connected\nlayer evaluation, which induces expensive memory trans-\nfer between GPU and CPU. Secondly, the Faster R-CNN\napproach uses convolutional computation for the evaluatio n\nof anchor regions, which is highly optimized compared to\nthe RoI pooling technique we adopted. Despite these in-\nef\ufb01ciencies, our approach still achieves high accuracy at a\nstate-of-the-art frame rate, using lower-end hardware. Wi th\nimproved implementation and model design we expect our\nalgorithm to be signi\ufb01cantly faster.\nAn interesting aspect that highlights the advantages of\nour approach is the small number of anchor regions to eval-\nuate. To further understand this aspect of our algorithm,\nwe show in Figure 12 the distribution of anchor regions\nevaluated for each image. For most images our method\nonly requires a few dozen anchor regions. This number is\nmuch smaller than the 2400 anchor regions used in RPN\n[22] and the 800 used in MultiBox [5]. Future work could\nfurther capitalize on this advantage by using an expensive\nbut more accurate per-anchor step, or by exploring applica-\ntions to very high-resolution images, for which traditiona l\nnon-adaptive approaches will face intrinsic dif\ufb01culties d ue\nto scalability issues. Our experiment also demonstrates th e\npossibility of designing a class-generic search. Unlike pe r-\nclass search methods widely used in previous adaptive ob-\nject detection schemes [3, 28] our anchor regions are shared\namong object classes, making it ef\ufb01cient for multi-class de -\ntection.\n4.4. Results on MSCOCO\nWe also evaluated our method on MSCOCO dataset and\nsubmitted a \u201cUCSD\u201d entry to the MSCOCO 2015 detection\nchallenge. Our post-competition work greatly improved ac-\ncuracy with more training iterations. A comparison with\nother recent methods is shown in Table 3. Our model isMethod AP AP IoU=0.50\nFRCNN (VGG16) [9] 19.7 35.9\nFRCNN (VGG16) [22] 19.3 39.3\nRPN (VGG16) 21.9 42.7\nRPN (ResNet) 37.4 59.0\nAZ-Net (VGG16) 22.3 41.0\nTable 3. The detection mAP on MSCOCO 2015 test-dev set. The\nRPN (ResNet) entry won the MSCOCO 2015 detection challenge.\nUpdated leaderboard can be found in http://mscoco.org .\ntrained with minibatches consisting of 256 regions sampled\nfrom one image, and 720k iterations in total. The results\nfor RPN(VGG16) reported in [22] were obtained with an\n8-GPU implementation that effectively has 8 and 16 images\nper minibatch for RPN and Fast R-CNN respectively, each\ntrained at 320k training iterations. Despite the much short er\neffective training iterations, our AZ-Net achieves simila r\nmAP with RPN(VGG16) and is more accurate when eval-\nuated on the MSCOCO mAP metric that rewards accurate\nlocalization.\nOur best post-competition model is still signi\ufb01cantly\noutperformed by the winning \u201cMSRA\u201d entry. Their ap-\nproach is a Faster-R-CNN-style detection pipeline, replac -\ning the VGG-16 network with an ultra-deep architecture\ncalled Deep Residual Network [13]. They also report signif-\nicant improvement from using model ensembles and global\ncontextual information. We note that these developments\nare complementary to our contribution.\n5. Conclusion and Future Work\nThis paper has introduced an adaptive object detection\nsystem using adjacency and zoom predictions. Our al-\ngorithm adaptively focuses its computational resources on\nsmall regions likely to contain objects, and demonstrates\nstate-of-the-art accuracy at a fast frame rate.\nThe current method can be further extended and im-\nproved in many aspects. Better pre-trained models [13]\ncan be incorporated into the current system for even bet-\nter accuracy. Further re\ufb01ning the model to allow single-\npipeline detection that directly predicts class labels, as in\nYOLO [21] and the more recent SSD [19] method, could\nsigni\ufb01cantly boost testing frame rate. Recent techniques\nthat improve small object detection, such as the contextual\nmodel and skip layers adopted in Inside-Outside Net [1],\nsuggest additional promising directions. It is also intere st-\ning to consider more aggressive extensions. For instance, i t\nmight be advantageous to use our search structure to focus\nhigh-resolution convolutional layer computation on small er\nregions, especially for very high-resolution images.\nAcknowledgment\nThis work is supported by the National Science Foun-\ndation grants CIF-1302438, CCF-1302588 and CNS-\n1329819, as well as Xerox UAC, and the Sloan Foundation.\n2358\n\nReferences\n[1] S. Bell, C. L. Zitnick, K. Bala, and R. Girshick. Inside-\noutside net: Detecting objects in context with skip\npooling and recurrent neural networks. arXiv preprint\narXiv:1512.04143 , 2015.\n[2] A. Borji and L. Itti. State-of-the-art in visual attenti on model-\ning.Pattern Analysis and Machine Intelligence, IEEE Trans-\nactions on , 35(1):185\u2013207, 2013.\n[3] J. C. Caicedo and S. Lazebnik. Active object localizatio n\nwith deep reinforcement learning. In The IEEE International\nConference on Computer Vision (ICCV) , December 2015.\n[4] S. K. Divvala, D. Hoiem, J. H. Hays, A. Efros, M. Hebert,\net al. An empirical study of context in object detection.\nInComputer Vision and Pattern Recognition, 2009. CVPR\n2009. IEEE Conference on , pages 1271\u20131278. IEEE, 2009.\n[5] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalabl e\nobject detection using deep neural networks. In Computer\nVision and Pattern Recognition (CVPR), 2014 IEEE Confer-\nence on , pages 2155\u20132162. IEEE, 2014.\n[6] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\nand A. Zisserman. The PASCAL Visual Object Classes\nChallenge 2007 (VOC2007) Results. http://www.pascal-\nnetwork.org/challenges/VOC/voc2007/workshop/index.h tml.\n[7] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\nand A. Zisserman. The PASCAL Visual Object Classes\nChallenge 2012 (VOC2012) Results. http://www.pascal-\nnetwork.org/challenges/VOC/voc2012/workshop/index.h tml.\n[8] S. Gidaris and N. Komodakis. Object detection via a multi -\nregion and semantic segmentation-aware cnn model. In The\nIEEE International Conference on Computer Vision (ICCV) ,\nDecember 2015.\n[9] R. Girshick. Fast r-cnn. In The IEEE International Confer-\nence on Computer Vision (ICCV) , December 2015.\n[10] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich f ea-\nture hierarchies for accurate object detection and semanti c\nsegmentation. In Computer Vision and Pattern Recognition\n(CVPR), 2014 IEEE Conference on , pages 580\u2013587. IEEE,\n2014.\n[11] A. Gonzalez-Garcia, A. Vezhnevets, and V . Ferrari. An a c-\ntive search strategy for ef\ufb01cient object class detection. I n\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , pages 3022\u20133031, 2015.\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pool-\ning in deep convolutional networks for visual recognition.\nInComputer Vision\u2013ECCV 2014 , pages 346\u2013361. Springer,\n2014.\n[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-\ning for image recognition. arXiv preprint arXiv:1512.03385 ,\n2015.\n[14] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\nshick, S. Guadarrama, and T. Darrell. Caffe: Convolu-\ntional architecture for fast feature embedding. arXiv preprint\narXiv:1408.5093 , 2014.\n[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\nclassi\ufb01cation with deep convolutional neural networks. In\nAdvances in neural information processing systems , pages\n1097\u20131105, 2012.[16] C. H. Lampert, M. B. Blaschko, and T. Hofmann. Ef\ufb01cient\nsubwindow search: A branch and bound framework for ob-\nject localization. Pattern Analysis and Machine Intelligence,\nIEEE Transactions on , 31(12):2129\u20132142, 2009.\n[17] X. Liang, S. Liu, Y . Wei, L. Liu, L. Lin, and S. Yan. To-\nwards computational baby learning: A weakly-supervised\napproach for object detection. In The IEEE International\nConference on Computer Vision (ICCV) , December 2015.\n[18] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. R a-\nmanan, P. Doll\u00b4 ar, and C. L. Zitnick. Microsoft coco: Com-\nmon objects in context. In Computer Vision\u2013ECCV 2014 ,\npages 740\u2013755. Springer, 2014.\n[19] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed.\nSsd: Single shot multibox detector. arXiv preprint\narXiv:1512.02325 , 2015.\n[20] Y . Lu and T. Javidi. Ef\ufb01cient object detection for high r eso-\nlution images. arXiv preprint arXiv:1510.01257 , 2015.\n[21] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You\nonly look once: Uni\ufb01ed, real-time object detection. arXiv\npreprint arXiv:1506.02640 , 2015.\n[22] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towa rds\nreal-time object detection with region proposal networks. In\nC. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and\nR. Garnett, editors, Advances in Neural Information Process-\ning Systems 28 , pages 91\u201399. Curran Associates, Inc., 2015.\n[23] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556 , 2014.\n[24] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed,\nD. Anguelov, D. Erhan, V . Vanhoucke, and A. Rabi-\nnovich. Going deeper with convolutions. arXiv preprint\narXiv:1409.4842 , 2014.\n[25] C. Szegedy, A. Toshev, and D. Erhan. Deep neural network s\nfor object detection. In Advances in Neural Information Pro-\ncessing Systems , pages 2553\u20132561, 2013.\n[26] A. Torralba, A. Oliva, M. S. Castelhano, and J. M. Hender -\nson. Contextual guidance of eye movements and attention\nin real-world scenes: the role of global features in object\nsearch. Psychological review , 113(4):766, 2006.\n[27] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W.\nSmeulders. Selective search for object recognition. Interna-\ntional journal of computer vision , 104(2):154\u2013171, 2013.\n[28] D. Yoo, S. Park, J.-Y . Lee, A. S. Paek, and I. So Kweon. At-\ntentionnet: Aggregating weak directions for accurate obje ct\ndetection. In The IEEE International Conference on Com-\nputer Vision (ICCV) , December 2015.\n[29] C. L. Zitnick and P. Doll\u00b4 ar. Edge boxes: Locating objec t pro-\nposals from edges. In Computer Vision\u2013ECCV 2014 , pages\n391\u2013405. Springer, 2014.\n2359\n\n"
}